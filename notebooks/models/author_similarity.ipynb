{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c747a9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T10:53:28.412297Z",
     "start_time": "2022-06-17T10:53:21.718305Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# define cache for sentencebert\n",
    "os.environ['XDG_CACHE_HOME'] = 'home/msds2022/plarosa/ .cache'\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a6da3af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T10:53:30.074247Z",
     "start_time": "2022-06-17T10:53:28.422861Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/cleaned/research_profile_updated.xlsx')\n",
    "\n",
    "df = df.loc[:, ['Research Title', 'Author', 'Abstract', \n",
    "           'University (Abbreviation)', 'Campus']]\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20cc5750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T10:53:30.231858Z",
     "start_time": "2022-06-17T10:53:30.100637Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('author_sim_db.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51ce807e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T10:53:49.355931Z",
     "start_time": "2022-06-17T10:53:34.802296Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name home/msds2022/plarosa/ .cache/torch/sentence_transformers/allenai_specter. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "# load sentence transformer model\n",
    "# model = SentenceTransformer('allenai/scibert_scivocab_uncased')\n",
    "model = SentenceTransformer('allenai/specter')\n",
    "model.max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c0d5730c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T14:10:59.443151Z",
     "start_time": "2022-06-07T14:10:53.405155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for computing embeddings: 6.022053241729736\n"
     ]
    }
   ],
   "source": [
    "# encode abstract using pretrained model\n",
    "start_time = time.time()\n",
    "embeddings = model.encode(df.Abstract.tolist(), convert_to_tensor=True)\n",
    "end_time = time.time()\n",
    "print(\"Time for computing embeddings:\", str(end_time - start_time))\n",
    "abstract_database = embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f9fcf418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T14:16:32.450439Z",
     "start_time": "2022-06-07T14:16:32.432028Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('abs_embed_db.pickle', 'wb') as f:\n",
    "    pickle.dump(abstract_database, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b17b361",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5936a3ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T11:05:09.229144Z",
     "start_time": "2022-06-17T11:05:08.437376Z"
    }
   },
   "outputs": [],
   "source": [
    "n_sim = 5\n",
    "abs_text = \"Investigating congestion in train rapid transit systems (RTS) in today's urban cities is a challenge compounded by limited data availability and difficulties in model validation. Here, we integrate information from travel smart card data, a mathematical model of route choice, and a full-scale agent-based model of the Singapore RTS to provide a more comprehensive understanding of the congestion dynamics than can be obtained through analytical modelling alone. Our model is empirically validated, and allows for close inspection of congestion and scaling dynamics. By adjusting our model, we can estimate the effective capacity of the RTS trains as well as replicate the penultimate station effect, where commuters travel backwards to the preceding station to catch a seat, sacrificing time for comfort. Using current data, the crowdedness in all 121 stations appears to be distributed log-normally. We find that increasing the current population (2 million) beyond a factor of approximately 10% leads to an exponential deterioration in service quality. We also show that incentivizing commuters to avoid the most congested hours can bring modest improvements to the service quality. Finally, our model can be used to generate simulated data for statistical analysis when such data are not empirically available, as is often the case.\"\n",
    "def get_most_similar_research(abs_text, n_sim=1):\n",
    "    with open('models/specter.pickle', 'rb') as f:\n",
    "        encoder = pickle.load(f)\n",
    "\n",
    "    author_sim_db = pd.read_csv('author_sim_db.csv')   \n",
    "\n",
    "    with open('abs_embed_db.pickle', 'rb') as f:\n",
    "        abs_embed_db = pickle.load(f)    \n",
    "\n",
    "\n",
    "    abs_emb = encoder.encode(abs_text)\n",
    "    scores = util.cos_sim(abs_emb, abs_embed_db).numpy()\n",
    "    arg_scores = scores[0].argsort(kind='mergesort')[::-1]\n",
    "    top_n_sim = author_sim_db.iloc[arg_scores[:n_sim], :]\n",
    "\n",
    "    if n_sim == 1:\n",
    "        return {'most_sim_auth' : top_n_sim.Author.values[0], \n",
    "                'most_sim_res' : top_n_sim['Research Title'].values[0], \n",
    "                'most_sim_school' : (top_n_sim['University (Abbreviation)'].values[0]\n",
    "                             + ' (' + top_n_sim['Campus'].values[0]\n",
    "                             + ')')} \n",
    "    else:\n",
    "        return top_n_sim\n",
    "    \n",
    "out = get_most_similar_research(abs_text, n_sim=n_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "675d41fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T11:00:29.324382Z",
     "start_time": "2022-06-17T11:00:29.313219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Research Title               Lum√°wig: An Efficient Algorithm for Dimension...\n",
       "Author                       Ignacio, Paul Samuel;Bulauan, Jay-Anne;Uminsky...\n",
       "Abstract                     Stability of persistence diagrams under slight...\n",
       "University (Abbreviation)                                            UP-Baguio\n",
       "Campus                                                                  Baguio\n",
       "Name: 631, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74203d7",
   "metadata": {},
   "source": [
    "## Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c067d828",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T14:27:38.604761Z",
     "start_time": "2022-06-07T14:27:38.514625Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = df['Abstract'][0]\n",
    "sample_emb = model.encode(sample, convert_to_tensor=True).cpu().numpy()\n",
    "cos_score = util.cos_sim(sample_emb, abstract_database)\n",
    "dot_score = util.dot_score(sample_emb, abstract_database)\n",
    "semantic_search = util.semantic_search(sample_emb, abstract_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7bbe69ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T14:30:55.817814Z",
     "start_time": "2022-06-07T14:30:55.810124Z"
    }
   },
   "outputs": [],
   "source": [
    "df['score'] = cos_score[0].cpu().numpy()\n",
    "df['dot_score'] = dot_score[0].cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
